{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tvo22\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, Trainer, TrainingArguments, BertModel\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_LSTM_Conv_Fc(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(Bert_LSTM_Conv_Fc, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=256, num_layers=3, batch_first=True, bidirectional=False)\n",
    "        self.conv1d = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3)\n",
    "        \n",
    "        # Chỉnh sửa kích thước fc1 và fc2 để phù hợp\n",
    "        self.fc1 = nn.Linear(128, 128)  # Đầu vào và đầu ra của fc1 cùng là 128\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # Đầu ra của fc2 phù hợp với số lớp\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        bert_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        lstm_output, _ = self.lstm(bert_output)  # [batch_size, seq_length, 256]\n",
    "        lstm_output = lstm_output.permute(0, 2, 1)  # [batch_size, 256, seq_length]\n",
    "        conv_out = self.conv1d(lstm_output)  # [batch_size, 128, seq_length-2] (do kernel_size=3)\n",
    "        x_out = conv_out.mean(dim=2)  # [batch_size, 128]\n",
    "        x = self.fc1(x_out)  # [batch_size, 128]\n",
    "        logits = self.fc2(x)  # [batch_size, num_classes]\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "with open('/kaggle/input/semeval/train_text.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  train_texts = []\n",
    "  for line in data:\n",
    "    train_texts.append(line.strip())\n",
    "\n",
    "with open('/kaggle/input/semeval/train_labels.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  train_labels = []\n",
    "  for line in data:\n",
    "    train_labels.append(int(line.strip()))\n",
    "\n",
    "with open('/kaggle/input/semeval/val_text.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  val_texts = []\n",
    "  for line in data:\n",
    "    val_texts.append(line.strip())\n",
    "\n",
    "with open('/kaggle/input/semeval/val_labels.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  val_labels = []\n",
    "  for line in data:\n",
    "    val_labels.append(int(line.strip()))\n",
    "\n",
    "with open('/kaggle/input/semeval/test_text.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  test_texts = []\n",
    "  for line in data:\n",
    "    test_texts.append(line.strip())\n",
    "\n",
    "with open('/kaggle/input/semeval/test_labels.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  test_labels = []\n",
    "  for line in data:\n",
    "    test_labels.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing text\n",
    "# remove emoji\n",
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# lower case\n",
    "def lower_case(text:str):\n",
    "    return text.lower()\n",
    "\n",
    "# remove @user\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)    \n",
    "\n",
    "def remove_tag(text: str):\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def clean_text(data_text: list):\n",
    "  clean = []\n",
    "  for text in data_text:\n",
    "    text = remove_emojis(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_tag(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html_tags(text)\n",
    "\n",
    "    clean.append(text)\n",
    "  return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert_lstm_fc = Bert_LSTM_Conv_Fc(bert_model, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "train_clean = clean_text(train_texts)\n",
    "val_clean = clean_text(val_texts)\n",
    "test_clean = clean_text(test_texts)\n",
    "\n",
    "train_dataset = CustomDataset(train_clean, train_labels, tokenizer, max_length=max_length)\n",
    "val_dataset = CustomDataset(val_clean, val_labels, tokenizer, max_length=max_length)\n",
    "test_dataset = CustomDataset(test_clean, test_labels, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Tải metric tính toán accuracy\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Khởi tạo Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_lstm_fc,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics  # Thêm hàm đánh giá tuỳ chỉnh\n",
    ")\n",
    "# Huấn luyện mô hình\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Tính toán ma trận nhầm lẫn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(labels, preds)\n",
    "# print(\"Accuracy: {}\".format(accuracy_score(test_labels, preds)))\n",
    "# # Hiển thị ma trận nhầm lẫn\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./best_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
