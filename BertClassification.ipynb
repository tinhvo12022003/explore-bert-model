{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, Trainer, TrainingArguments\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "with open('', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  train_texts = []\n",
    "  for line in data:\n",
    "    train_texts.append(line.strip())\n",
    "\n",
    "with open('', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  train_labels = []\n",
    "  for line in data:\n",
    "    train_labels.append(int(line.strip()))\n",
    "\n",
    "with open('', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  val_texts = []\n",
    "  for line in data:\n",
    "    val_texts.append(line.strip())\n",
    "\n",
    "with open('', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  val_labels = []\n",
    "  for line in data:\n",
    "    val_labels.append(int(line.strip()))\n",
    "\n",
    "with open('', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  test_texts = []\n",
    "  for line in data:\n",
    "    test_texts.append(line.strip())\n",
    "\n",
    "with open('', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  test_labels = []\n",
    "  for line in data:\n",
    "    test_labels.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing text\n",
    "# remove emoji\n",
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# lower case\n",
    "def lower_case(text:str):\n",
    "    return text.lower()\n",
    "\n",
    "# remove @user\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)    \n",
    "\n",
    "def remove_tag(text: str):\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def clean_text(data_text: list):\n",
    "  clean = []\n",
    "  for text in data_text:\n",
    "    text = remove_emojis(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_tag(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html_tags(text)\n",
    "\n",
    "    clean.append(text)\n",
    "  return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=max_length)\n",
    "test_dataset = CustomDataset(test_texts, test_labels, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Tải metric tính toán accuracy\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='',\n",
    "    logging_steps=200,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Khởi tạo Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Tính toán ma trận nhầm lẫn\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(test_labels, preds)))\n",
    "# Hiển thị ma trận nhầm lẫn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['auto'], yticklabels=['auto'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./best_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
