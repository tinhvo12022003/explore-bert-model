{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tvo22\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments, XLNetTokenizer, XLNetModel\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetRCNN(nn.Module):\n",
    "    def __init__(self, xlnet_model_name='xlnet/xlnet-base-cased', device='cpu'):\n",
    "        super(XLNetRCNN, self).__init__()\n",
    "        self.xlnet_model = XLNetModel.from_pretrained(xlnet_model_name)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=256,\n",
    "            num_layers=3,\n",
    "            bias=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.2,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=128, out_features=3)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.xlnet_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = outputs.last_hidden_state\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.mean(dim=2)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        # Nếu có nhãn (labels), tính toán và trả về loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = XLNetRCNN(device=device)\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet/xlnet-base-cased')\n",
    "max_length = 512\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "with open('/kaggle/input/semeval/train_text.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  train_texts = []\n",
    "  for line in data:\n",
    "    train_texts.append(line.strip())\n",
    "\n",
    "with open('/kaggle/input/semeval/train_labels.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  train_labels = []\n",
    "  for line in data:\n",
    "    train_labels.append(int(line.strip()))\n",
    "\n",
    "with open('/kaggle/input/semeval/val_text.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  val_texts = []\n",
    "  for line in data:\n",
    "    val_texts.append(line.strip())\n",
    "\n",
    "with open('/kaggle/input/semeval/val_labels.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  val_labels = []\n",
    "  for line in data:\n",
    "    val_labels.append(int(line.strip()))\n",
    "\n",
    "with open('/kaggle/input/semeval/test_text.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  test_texts = []\n",
    "  for line in data:\n",
    "    test_texts.append(line.strip())\n",
    "\n",
    "with open('/kaggle/input/semeval/test_labels.txt', 'r') as f:\n",
    "  data = f.readlines()\n",
    "  test_labels = []\n",
    "  for line in data:\n",
    "    test_labels.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing text\n",
    "# remove emoji\n",
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# lower case\n",
    "def lower_case(text:str):\n",
    "    return text.lower()\n",
    "\n",
    "# remove @user\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)    \n",
    "\n",
    "def remove_tag(text: str):\n",
    "    return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def clean_text(data_text: list):\n",
    "  clean = []\n",
    "  for text in data_text:\n",
    "    text = remove_emojis(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_tag(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html_tags(text)\n",
    "\n",
    "    clean.append(text)\n",
    "  return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "train_clean = clean_text(train_texts)\n",
    "val_clean = clean_text(val_texts)\n",
    "test_clean = clean_text(test_texts)\n",
    "\n",
    "train_dataset = CustomDataset(train_clean, train_labels, tokenizer, max_length=max_length)\n",
    "val_dataset = CustomDataset(val_clean, val_labels, tokenizer, max_length=max_length)\n",
    "test_dataset = CustomDataset(test_clean, test_labels, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Khởi tạo Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./best_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
